[{"title":"相关基础环境安装","date":"2023-03-30T16:00:00.000Z","url":"/2023/03/31/%E6%8A%80%E6%9C%AF/%E5%90%8E%E7%AB%AF/%E7%9B%B8%E5%85%B3%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85/%E7%9B%B8%E5%85%B3%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85/","tags":[["java","/tags/java/"]],"categories":[["java","/categories/java/"]],"content":"相关环境安装 记录相关环境安装基础步骤。 jdk-1.8 nginx 1 jdk 安装步骤1.1 创建目录存放 jdk 安装包 1.2 去华为云找自己想要的jdk版本 1.3 解压缩 1.4 配置环境变量 添加以下内容 1.5 刷新环境变量 1.6 查看是否安装成功 1.7 设置软连接 为什么要建这个超链接，因为一些自己注册的linux服务（如springboot的jar注册的服务），默认情况下从&#x2F;usr&#x2F;bin&#x2F;java路径使用javayum安装的时候，这个超链接会自动创建，如果你自己下载包安装的话，这个超链接就需要你手动创建了。 2 nginx 安装2.1 创建路径 2.2 安装 gcc 因为Nginx依赖于gcc的编译环境，所以，需要安装编译环境来使Nginx能够编译起来。 2.3 安装相关依赖 Nginx的http模块需要使用pcre来解析正则表达式，需要安装pcre。安装依赖的解压包。ssl 功能需要 openssl 库，安装 openssl。 2.4 进入 nginx 官网，下载指定版本 nginx Mainline version：Mainline 是 Nginx 目前主力在做的版本，可以说是开发版 Stable version：最新稳定版，生产环境上建议使用的版本 Legacy versions：遗留的老版本的稳定版 我们选择Stable version，点击下载。下载完成后，将Nginx压缩包移动到Linux的待安装目录中。我这里是 &#x2F;usr&#x2F;local&#x2F;nginx 2.5 进行解压 2.6 执行相关命令 2.7 启动 nginx nginx 端口默认 80, 在 &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;nginx-1.22.1&#x2F;conf&#x2F;nginx.conf 中可以修改 "},{"title":"Docker->安装和基本命令","date":"2023-03-23T16:00:00.000Z","url":"/2023/03/24/%E6%8A%80%E6%9C%AF/%E5%90%8E%E7%AB%AF/docker/docker%E7%9A%84%E5%AE%89%E8%A3%85%E5%92%8C%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4/","tags":[["Docker","/tags/Docker/"]],"categories":[["docker","/categories/docker/"]],"content":"docker 的基本使用命令 参考： 1. 安装 rabbitmq 端口开放 1.1 拉取 rabbitmq 镜像 这里拉取的最新版本， 可以根据实际需求下载其它版本 1.2 启动MQ安装management 本条命令包括安装Web页面管理的 rabbitmq:management组件，账号和密码都为 admin ；-p 后面参数表示公网IP地址的端口号对应容器内部的端口号 1.3 访问 ip:15672 即可。 账户名： admin 密码： admin 2. 安装 kafka2.1 安装zookeeper Kafka依赖zookeeper所以先安装zookeeper-p：设置映射端口（默认2181）-d：后台启动 2.2 安装 kafka KAFKA_BROKER_ID&#x3D;0 在kafka集群中，每个kafka都有一个BROKER_ID来区分自己KAFKA_ZOOKEEPER_CONNECT&#x3D;110.40.135.78:2181&#x2F;kafka 配置zookeeper管理kafka的路径KAFKA_ADVERTISED_LISTENERS&#x3D;PLAINTEXT:&#x2F;&#x2F;110.40.135.78:9092 把kafka的地址端口注册给zookeeperKAFKA_LISTENERS&#x3D;PLAINTEXT:&#x2F;&#x2F;0.0.0.0:9092 配置kafka的监听端口 2.3 kafka-map图形化管理工具 图形化管理工具访问地址：http:&#x2F;&#x2F;服务器IP:9001&#x2F;DEFAULT_USERNAME：默认账号adminDEFAULT_PASSWORD：默认密码adminGit 地址： 2.4 访问 ip:9001 即可。 账户名： admin 密码： admin "},{"title":"hexo->个人博客搭建","date":"2022-02-13T16:00:00.000Z","url":"/2022/02/14/%E6%8A%80%E6%9C%AF/other/hexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/","tags":[["hexo","/tags/hexo/"]],"categories":[["hexo","/categories/hexo/"]],"content":"个人博客搭建一、下载安装 git ，Node.jsNode.js下载地址：： 二、安装，配置 Hexo安装: 初始化项目: 配置: (1)新建存放博客文件夹 (2)进入文件夹，并且打开Git Bash (3)运行$hexo init$ npm install _config.yml 可以修改我们的博客标题，作者，邮箱，网址 … .. 三、本地启动Hexo$hexo g # 生成$hexo s #启动本地服务器,这一步之后就可以通过 查看 浏览器输入： 四、将Hexo部署到GitHub上1.注册GitHub账户（默认大家都有了） （无法访问就在微软自带的浏览器安装 Hoxx Vpn 插件）2.新建仓库3.使用Hexo deploy 部署到GitHub3.1 编辑根目录下_config.yml文件 DeploymentDocs: : type: git repo: #仓库地址 branch: main3.2 安装扩展npm install hexo-deployer-git –save 3.3 设置用户信息$ git config –global user.name “注册时候的用户名” #自己的用户名 username$ git config –global user.email “邮件地址@youremail.com” #填写Git的邮箱 email3.4 添加SSh Key到GitHub上ssh-keygen -t rsa -C “邮件地址@youremail.com” # 生成Key email 在 C:&#x2F;User&#x2F;username&#x2F;.ssh&#x2F;文件里面ssh -T &#x67;&#105;&#x74;&#x40;&#103;&#105;&#x74;&#x68;&#117;&#x62;&#x2e;&#x63;&#111;&#109; # 测试我们的Key是否添加成功3.5 部署到Github 上 $hexo d"},{"title":"ES->进阶型原理","date":"2022-02-13T16:00:00.000Z","url":"/2022/02/14/%E6%8A%80%E6%9C%AF/%E5%90%8E%E7%AB%AF/ElasticSearch/elasticsearch-theory/","tags":[["ElasticSearch","/tags/ElasticSearch/"]],"categories":[["database","/categories/database/"]],"content":"进阶型原理 传统关系型数据库的行和列存储，这相当于是把一个表现力丰富的对象挤压到一个非常大的电子表格中：你必须将这个对象扁平化来适应表结构–通常一个字段&gt;对应一列–而且又不得不在每次查询时重新构造对象。 Elasticsearch 是 面向文档 的，意味着它存储整个对象或 文档_。Elasticsearch 不仅存储文档，而且 _索引 每个文档的内容使之可以被检索。在 Elasticsearch 中，你 对文档进行索引、检索、排序和过滤–而不是对行列数据。这是一种完全不同的思考数据的方式，也是 Elasticsearch 能支持复杂全文检索的原因。 集群相关 节点、主节点、主分片、副本分片 一个运行中的 Elasticsearch 实例称为一个 节点，而集群是由一个或者多个拥有相同 cluster.name 配置的节点组成， 它们共同承担数据和负载的压力。当有节点加入集群中或者从集群中移除节点时，集群将会重新平均分布所有的数据。 当一个节点被选举成为主节点时， 它将负责管理集群范围内的所有变更，例如增加、删除索引，或者增加、删除节点等。 而主节点并不需要涉及到文档级别的变更和搜索等操作，所以当集群只拥有一个主节点的情况下，即使流量的增加它也不会成为瓶颈。 任何节点都可以成为主节点。集群状态存在于集群中的每个节点，包括客户端节点。但只有主节点被允许更新集群状态，然后向所有集群中的所有节点发布一个新的版本。 每个节点上都有若干分片（即可以有主分片也可以有副分片）。 作为用户，我们可以将请求发送到集群中的任何节点，包括主节点。 每个节点都知道任意文档所处的位置，并且能够将我们的请求直接转发到存储我们所需文档的节点。 无论我们将请求发送到哪个节点，它都能负责从各个包含我们所需文档的节点收集回数据，并将最终结果返回給客户端。 Elasticsearch 对这一切的管理都是透明的。 集群健康命令： curl -XGET ‘localhost:9200&#x2F;_cluster&#x2F;health?pretty’ Elasticsearch 是利用分片将数据分发到集群内各处的。分片是数据的容器，文档保存在分片内，分片又被分配到集群内的各个节点里。 当你的集群规模扩大或者缩小时， Elasticsearch 会自动的在各节点中迁移分片，使得数据仍然均匀分布在集群里。 在索引建立的时候就已经确定了主分片数，但是副本分片数可以随时修改。 水平扩容 创建更多的副本分片，提供服务（读操作——搜索和返回数据——可以同时被主分片 或 副本分片所处理），提高系统的吞吐量。  分布式文档存储当我们创建文档时，被存储到哪一个主分片？有一个公式 新建、更新和删除 请求都是 写 操作， 必须在主分片上面完成之后才能被复制到相关的副本分片。 **当主分片把更改转发到副本分片时， 它不会转发更新请求。 相反，它转发完整文档的新版本。**  读取请求，协调节点在每次请求的时候将选择不同的副本分片来达到负载均衡；通过轮询所有的副本分片。 注：在分布式系统中，对结果排序的成本随分页的深度成指数上升。这就是 web 搜索引擎对任何查询都不要返回超过 1000 个结果的原因。 倒排索引Elasticsearch 使用一种称为 倒排索引 的结构，它适用于快速的全文搜索。一个倒排索引由文档中所有不重复词的列表构成，对于其中每个词，有一个包含它的文档列表。 分布式检索当一个节点接收搜索请求时，这个节点就变成了协调节点。 这个节点的任务是广播查询请求到所有相关分片并将它们的响应整合成全局排序后的结果集合，这个结果集合会返回给客户端。 对于楼梯式的翻页，随着页号越大，效率越差。每个分片在本地执行查询请求并且创建一个长度为 from + size 的优先队列，也就是说，每个分片创建的结果集足够大，均可以满足全局的搜索请求。 分片返回一个轻量级的结果列表到协调节点，它仅包含文档 ID 集合以及任何排序需要用到的值，例如 _score 。协调节点将这些分片级的结果合并到自己的有序优先队列里，它代表了全局排序结果集合。至此查询过程结束。 过滤器例子： 在内部，Elasticsearch 会在运行非评分查询的时执行多个操作： 查找匹配文档. term 查询在倒排索引中查找 XHDK-A-1293-#fJ3 然后获取包含该 term 的所有文档。本例中，只有文档 1 满足我们要求。 创建 bitset. 过滤器会创建一个 bitset （一个包含 0 和 1 的数组），它描述了哪个文档会包含该 term 。每一个文档都会有一个二进制数字。匹配上的文档的标志位是 1 。本例中，bitset 的值为 [1,0,0,0] ，因为只有4个文档。在内部，它表示成一个 “roaring bitmap”，可以同时对稀疏或密集的集合进行高效编码。 bitset是一个标志位数组，表示有或没有，猜测应该有一个对应的文档id号的数组，数组的下标可以做关联。 迭代 bitset(s) 一旦为每个查询生成了 bitsets ，Elasticsearch 就会循环迭代 bitsets 从而找到满足所有过滤条件的匹配文档的集合。执行顺序是启发式的，但一般来说先迭代稀疏的 bitset （因为它可以排除掉大量的文档）。 理论上非评分查询先于评分查询执行。非评分查询任务旨在降低那些将对评分查询计算带来更高成本的文档数量，从而达到快速搜索的目的。从概念上记住非评分计算是首先执行的，这将有助于写出高效又快速的搜索请求。"},{"title":"arthas -> 基本使用","date":"2022-02-13T16:00:00.000Z","url":"/2022/02/14/%E6%8A%80%E6%9C%AF/%E5%90%8E%E7%AB%AF/arthas/arthas_%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/","tags":[["arthas","/tags/arthas/"]],"categories":[["java","/categories/java/"]],"content":"arthas 基本使用1、 arthas jar包获取地址 2、 开启方式3、 相关命令"},{"title":"java -> 基础面试题","date":"2022-02-13T16:00:00.000Z","url":"/2022/02/14/%E6%8A%80%E6%9C%AF/%E5%90%8E%E7%AB%AF/java/java-basic/","tags":[["basis","/tags/basis/"]],"categories":[["java","/categories/java/"]],"content":"java基础面试题 1、 &#x3D;&#x3D;和equals⽅法之前的区别 &#x3D;&#x3D;：对⽐的是栈中的值，基本数据类型是变量值，引⽤类型是堆中内存对象的地址equals：object中默认也是采⽤&#x3D;&#x3D;⽐较，通常会重写 2、 hashCode()与equals() 关系和区别 如果两个对象相等，则hashcode⼀定也是相同的 两个对象相等,对两个对象分别调⽤equals⽅法都返回true 两个对象有相同的hashcode值，它们也不⼀定是相等的 因此，equals⽅法被覆盖过，则hashCode⽅法也必须被覆盖hashCode()的默认⾏为是对堆上的对象产⽣独特值。如果没有重写hashCode()，则该class的两个对象⽆论如何都不会相等（即使这两个对象指向相同的数据） 3、 final关键字的作⽤是什么？ 修饰类：表示类不可被继承 修饰⽅法：表示⽅法不可被⼦类覆盖，但是可以重载 修饰变量：表示变量⼀旦被赋值就不可以更改它的值。 修饰成员变量：如果final修饰的是类变量，只能在静态初始化块中指定初始值或者声明该类变量时指定初始值。如果final修饰的是成员变量，可以在⾮静态初始化块、声明该变量或者构造器中执⾏初始值。 修饰局部变量： 系统不会为局部变量进⾏初始化，局部变量必须由程序员显示初始化。因此使⽤final修饰局部变量时，即可以在定义时指定默认值（后⾯的代码不能对变量再赋值），也可以不指定默认值，⽽在后⾯的代码中对final变量赋初值（仅⼀次） 修饰基本类型数据和引⽤类型数据： 如果是基本数据类型的变量，则其数值⼀旦在初始化之后便不能更改； 如果是引⽤类型的变量，则在对其初始化之后便不能再让其指向另⼀个对象。但是引⽤的值是可变的。 4、 String、StringBuffer、StringBuilder的区别 String是不可变的，如果尝试去修改，会新⽣成⼀个字符串对象，StringBuffer和StringBuilder是可变的 StringBuffer是线程安全的，StringBuilder是线程不安全的，所以在单线程环境下StringBuilder效率会更⾼ 5、重载和重写的区别 重载：发⽣在同⼀个类中，⽅法名必须相同，参数类型不同、个数不同、顺序不同，⽅法返回值和访问修饰符可以不同，发⽣在编译时。 重写：发⽣在⽗⼦类中，⽅法名、参数列表必须相同，返回值范围⼩于等于⽗类，抛出的异常范围⼩于等于⽗类，访问修饰符范围⼤于等于⽗类；如果⽗类⽅法访问修饰符为private则⼦类就不能重写该⽅法。 6、常见访问修饰符 1、private（私有）：private修饰的属性和方法，不能被其他到类访问，也不能被子类继承和访问，只能在当前类访问。 2、default （缺省）：没有加修饰符的属性和方法，同一个包的其他类可访问和继承。 3、protected（受保护的）：被其修饰的属性和方法，同一个包的其他类可访问和继承，或者不同包的其他子类可访问。 4、public（公有的）：不存在访问权限，全部类都可以访问。 7、接⼝和抽象类的区别 象类可以存在普通成员函数，⽽接⼝中只能存在public abstract ⽅法。 抽象类中的成员变量可以是各种类型的，⽽接⼝中的成员变量只能是public static final类型的。 抽象类只能继承⼀个，接⼝可以实现多个。 当你关注⼀个事物的本质的时候，⽤抽象类；当你关注⼀个操作的时候，⽤接⼝ 8、List和Set的区别 List: 有序，按对象进⼊的顺序保存对象，可重复，允许多个Null元素对象，可以使⽤Iterator取出所有元素，在逐⼀遍历，还可以使⽤get(int index)获取指定下标的元素 Set: ⽆序，不可重复，最多允许有⼀个Null元素对象，取元素时只能⽤Iterator接⼝取得所有元素，在逐⼀遍历各个元素 9、ArrayList和 LinkedList 区别 底层结构不同。 ArrayList 基于数组实现, 线程不安全。 插入的话会默认插到最后一位。 而且还要插入的时候考虑索引和数组大小。（例如自动扩容） 默认容量为 10 （DEFAULT_CAPACITY） 在用户向ArrayList追加对象时，Java总是要先计算容量（Capacity）是否适当，若容量不足则把原数组拷贝到以指定容量为长度创建的 新数组内，并对原数组变量重新赋值，指向新数组。 在这同时，size进行自增1。在删除对象时，先使用拷贝方法把指定index后面的对象前移1位（如果 有的话），然后把空出来的位置置null，交给Junk收集器销毁，size自减1。 LinkedList 基于链表实现 使用场景不同 如果应用程序对数据有较多的随机访问，ArrayList对象要优于LinkedList对象； 如果应用程序有更多的插入或者删除操作，较少的数据读取，LinkedList对象要优于ArrayList对象； 不过ArrayList的插入，删除操作也不一定比LinkedList慢，如果在List靠近末尾的地方插入，那么ArrayList只需要移动较少的数据，而LinkedList则需要一直查找到列表尾部，反而耗费较多时间，这时ArrayList就比LinkedList要快。 10、HashMap 和 HashTable 有什么区别？其底层实现是什么？ 安全性不同 HashMap 无 synsynchronized 修饰。 现场不安全 HashTable 线程安全的 插入值的允许度不同 HashMap 允许 key、 value 为空 HashTable 不允许 key、 value 为空 底层实现 数组+链表实现，jdk8开始链表⾼度到8、数组⻓度超过64，链表转变为红⿊树，元素以内部类Node节点存在 计算key的hash值，⼆次hash然后对数组⻓度取模，对应到数组下标， 如果没有产⽣hash冲突(下标位置没有元素)，则直接创建Node存⼊数组， 如果产⽣hash冲突，先进⾏equal⽐较，相同则取代该元素，不同，则判断链表⾼度插⼊链表，链表⾼度达到8，并且数组⻓度到64则转变为红⿊树，⻓度低于6则将红⿊树转回链表 key为null，存在下标0的位置 11、谈 ConcurrentHashMap 的扩容机制 1.7 1.7版本的ConcurrentHashMap是基于Segment分段实现的 每个Segment相对于⼀个⼩型的HashMap 每个Segment内部会进⾏扩容，和HashMap的扩容逻辑类似 先⽣成新的数组，然后转移元素到新数组中 扩容的判断也是每个Segment内部单独判断的，判断是否超过阈值 1.8 1.8版本的ConcurrentHashMap不再基于Segment实现 当某个线程进⾏put时，如果发现ConcurrentHashMap正在进⾏扩容那么该线程⼀起进⾏扩容 如果某个线程put时，发现没有正在进⾏扩容，则将key-value添加到ConcurrentHashMap中，然后判断是否超过阈值，超过了则进⾏扩容 ConcurrentHashMap是⽀持多个线程同时扩容的 扩容之前也先⽣成⼀个新的数组 在转移元素时，先将原数组分组，将每组分给不同的线程来进⾏元素的转移，每个线程负责⼀组或多组的元素转移⼯作 12、Jdk1.7到Jdk1.8 HashMap 发⽣了什么变化 1.7中底层是数组+链表，1.8中底层是数组+链表+红⿊树，加红⿊树的⽬的是提⾼HashMap插⼊和查询整体效率 1.7中链表插⼊使⽤的是头插法，1.8中链表插⼊使⽤的是尾插法，因为1.8中插⼊key和value时需要判断链表元素个数，所以需要遍历链表统计链表元素个数，所以正好就直接使⽤尾插法 1.7中哈希算法⽐较复杂，存在各种右移与异或运算，1.8中进⾏了简化，因为复杂的哈希算法的⽬的就是提⾼散列性，来提供HashMap的整体效率，⽽1.8中新增了红⿊树，所以可以适当的简化哈希算法，节省CPU资源 13、泛型中 extends 和 super 的区别 子类 和 父类的区别 表示包括T在内的任何T的⼦类 表示包括T在内的任何T的⽗类 14、深拷⻉和浅拷⻉ 深拷⻉和浅拷⻉就是指对象的拷⻉，⼀个对象中存在两种类型的属性，⼀种是基本数据类型，⼀种是实例对象的引⽤。 浅拷贝不会存在新的内存地址，指向的实际是同一个内存地址。 深拷贝会对引用地址也进行赋值。实际在内存中的应用地址是不同的。 浅拷⻉: 只会拷⻉基本数据类型的值，以及实例对象的引⽤地址，并不会复制⼀份引⽤地址所指向的对象，也就是浅拷⻉出来的对象，内部的类属性指向的是同⼀个对象 深拷⻉: 既会拷⻉基本数据类型的值，也会针对实例对象的引⽤地址所指向的对象进⾏复制，深拷⻉出来的对象，内部的属性指向的不是同⼀个对象 15、CopyOnWriteArrayList 的相关解释说明 ⾸先CopyOnWriteArrayList内部也是⽤过数组来实现的，在向CopyOnWriteArrayList添加元素时，会复制⼀个新的数组，写操作在新数组上进⾏，读操作在原数组上进⾏ 并且，写操作会加锁，防⽌出现并发写⼊丢失数据的问题 写操作结束之后会把原数组 CopyOnWriteArrayList允许在写操作时来读取数据，⼤⼤提⾼了读的性能，因此适合读多写少的应⽤场景，但是CopyOnWriteArrayList会⽐较占内存，同时可能读到的数据不是实时最新的数据，所以不适合实时性要求很⾼的场景 16、java 中的异常体系 java中的所有异常都来⾃顶级⽗类Throwable。 Throwable下有两个⼦类Exception和Error。 Error是程序⽆法处理的错误，⼀旦出现这个错误，则程序将被迫停⽌运⾏。 Exception不会导致程序停⽌，⼜分为两个部分RunTimeException运⾏时异常和CheckedException检查异常。 RunTimeException常常发⽣在程序运⾏过程中，会导致程序当前线程执⾏失败。 CheckedException常常发⽣在程序编译过程中，会导致程序编译不通过。 17、 java 中的类加载器 jDK⾃带有三个类加载器：bootstrap ClassLoader、ExtClassLoader、AppClassLoader。 BootStrapClassLoader是ExtClassLoader的⽗类加载器，默认负责加载%JAVA_HOME%lib下的jar包和class⽂件。 ExtClassLoader是AppClassLoader的⽗类加载器，负责加载%JAVA_HOME%&#x2F;lib&#x2F;ext⽂件夹下的jar包和class类。 AppClassLoader是⾃定义类加载器的⽗类，负责加载classpath下的类⽂件。 18、 说说类加载器双亲委派模型JVM中存在三个默认的类加载器： BootstrapClassLoader ExtClassLoader AppClassLoader AppClassLoader的⽗加载器是ExtClassLoader，ExtClassLoader的⽗加载器是BootstrapClassLoader。 JVM在加载⼀个类时，会调⽤AppClassLoader的loadClass⽅法来加载这个类，不过在这个⽅法中，会先使⽤ExtClassLoader的loadClass⽅法来加载类，同样ExtClassLoader的loadClass⽅法中会先使⽤BootstrapClassLoader来加载类，如果BootstrapClassLoader加载到了就直接成功，如果BootstrapClassLoader没有加载到，那么ExtClassLoader就会⾃⼰尝试加载该类，如果没有加载到，那么则会由AppClassLoader来加载这个类。 所以，双亲委派指得是，JVM在加载类时，会委派给Ext和Bootstrap进⾏加载，如果没加载到才由⾃⼰进⾏加载。 19、 GC如何判断对象可以被回收 引⽤计数法：每个对象有⼀个引⽤计数属性，新增⼀个引⽤时计数加1，引⽤释放时计数减1，计数为0时可以回收 注意互相引用时, 引用计数法无法回收、 （例如 A 引用 B, B 引用 A。则他们互相引用计数为 1，无法被该机制回收） 可达性分析法：从 GC Roots 开始向下搜索，搜索所⾛过的路径称为引⽤链。当⼀个对象到 GCRoots 没有任何引⽤链相连时，则证明此对象是不可⽤的，那么虚拟机就判断是可回收对象。 GC Roots的对象有： 虚拟机栈(栈帧中的本地变量表）中引⽤的对象 ⽅法区中类静态属性引⽤的对象 ⽅法区中常量引⽤的对象 本地⽅法栈中JNI(即⼀般说的Native⽅法)引⽤的对象 可达性算法中的不可达对象并不是⽴即死亡的，对象拥有⼀次⾃我拯救的机会。对象被系统宣告死亡⾄少要经历两次标记过程：第⼀次是经过可达性分析发现没有与GC Roots相连接的引⽤链，第⼆次是在由虚拟机⾃动建⽴的Finalizer队列中判断是否需要执⾏finalize()⽅法。 当对象变成(GC Roots)不可达时，GC会判断该对象是否覆盖了finalize⽅法，若未覆盖，则直接将其回收。否则，若对象未执⾏过finalize⽅法，将其放⼊F-Queue队列，由⼀低优先级线程执⾏该队列中对象的finalize⽅法。执⾏finalize⽅法完毕后，GC会再次判断该对象是否可达，若不可达，则进⾏回收，否则，对象“复活”每个对象只能触发⼀次finalize()⽅法 由于finalize()⽅法运⾏代价⾼昂，不确定性⼤，⽆法保证各个对象的调⽤顺序，不推荐⼤家使⽤，建议遗忘它。 20、 线程共享区 堆区和⽅法区是所有线程共享的，栈、本地⽅法栈、程序计数器是每个线程独有的 21、 jvm 相关参数 和 排查方向 env 中 XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath&#x3D;&#x2F;usr&#x2F;local&#x2F;base 添加配置项监控 oomjprofiler 工具在线监控。wireshark 查看 dump 文件 22、 ⼀个对象从加载到JVM，再到被GC清除，都经历了什么过程？ ⽤户创建⼀个对象，JVM⾸先需要到⽅法区去找对象的类型信息。然后再创建对象。 JVM要实例化⼀个对象，⾸先要在堆当中先创建⼀个对象。-&gt; 半初始化状态 对象⾸先会分配在堆内存中新⽣代的Eden。然后经过⼀次Minor GC，对象如果存活，就会进⼊S区。在后续的每次GC中，如果对象⼀直存活，就会在S区来回拷⻉，每移动⼀次，年龄加1。-&gt; 多⼤年龄才会移⼊⽼年代？ 年龄最⼤15， 超过⼀定年龄后，对象转⼊⽼年代。 当⽅法执⾏结束后，栈中的指针会先移除掉。 堆中的对象，经过Full GC，就会被标记为垃圾，然后被GC线程清理掉。 23、 怎么确定一个对象是否是垃圾 引⽤计数法 通过引用进行计数，当没有计数时认为无引用 根可达算法 从引用对象向下寻找。找不到对象则认为是垃圾 24、JVM有哪些垃圾回收算法？ MarkSweep 标记清除算法：这个算法分为两个阶段，标记阶段：把垃圾内存标记出来，清除阶段：直接将垃圾内存回收。这种算法是⽐较简单的，但是有个很严重的问题，就是会产⽣⼤量的内存碎⽚。 Copying 拷⻉算法：为了解决标记清除算法的内存碎⽚问题，就产⽣了拷⻉算法。拷⻉算法将内存分为⼤⼩相等的两半，每次只使⽤其中⼀半。垃圾回收时，将当前这⼀块的存活对象全部拷⻉到另⼀半，然后当前这⼀半内存就可以直接清除。这种算法没有内存碎⽚，但是他的问题就在于浪费空间。⽽且，他的效率跟存货对象的个数有关。 MarkCompack 标记压缩算法：为了解决拷⻉算法的缺陷，就提出了标记压缩算法。这种算法在标记阶段跟标记清除算法是⼀样的，但是在完成标记之后，不是直接清理垃圾内存，⽽是将存活对象往⼀端移动，然后将端边界以外的所有内存直接清除。 25、 jvm 的垃圾回收器 新⽣代收集器： Serial ParNew Parallel Scavenge ⽼年代收集器： CMS Serial Old Parallel Old 整堆收集器： G1 26、 垃圾回收阶段 初始标记 标记出GCRoot直接引⽤的对象。STW 标记Region，通过RSet标记出上⼀个阶段标记的Region引⽤到的Old区Region。 并发标记阶段：跟CMS的步骤是差不多的。只是遍历的范围不再是整个Old区，⽽只需要遍历第⼆步标记出来的Region。 重新标记： 跟CMS中的重新标记过程是差不多的。 垃圾清理：与CMS不同的是，G1可以采⽤拷⻉算法，直接将整个Region中的对象拷⻉到另 ⼀个Region。⽽这个阶段，G1只选择垃圾较多的Region来清理，并不是完全清理。 27、什么是三⾊标记？ 三⾊标记：是⼀种逻辑上的抽象。将每个内存对象分成三种颜⾊： ⿊⾊：表示⾃⼰和成员变量都已经标记完毕。 灰⾊：⾃⼰标记完了，但是成员变量还没有完全标记完。 ⽩⾊：⾃⼰未标记完。 28、JVM参数有哪些？ JVM参数⼤致可以分为三类： 标注指令： -开头，这些是所有的HotSpot都⽀持的参数。可以⽤java -help 打印出来。 ⾮标准指令： -X开头，这些指令通常是跟特定的HotSpot版本对应的。可以⽤java -X 打印出来。 不稳定参数： -XX 开头，这⼀类参数是跟特定HotSpot版本对应的，并且变化⾮常⼤。详细的⽂档 资料⾮常少。在JDK1.8版本下，有⼏个常⽤的不稳定指令： java -XX:+PrintCommandLineFlags ： 查看当前命令的不稳定指令。 java -XX:+PrintFlagsInitial ： 查看所有不稳定指令的默认值。 java -XX:+PrintFlagsFinal： 查看所有不稳定指令最终⽣效的实际值。 29、 线程的⽣命周期？线程有⼏种状态 线程通常有五种状态，创建，就绪，运⾏、阻塞和死亡状态： 新建状态（New）：新创建了⼀个线程对象。 就绪状态（Runnable）：线程对象创建后，其他线程调⽤了该对象的start⽅法。该状态的线程位于可运⾏线程池中，变得可运⾏，等待获取CPU的使⽤权。 运⾏状态（Running）：就绪状态的线程获取了CPU，执⾏程序代码。 阻塞状态（Blocked）：阻塞状态是线程因为某种原因放弃CPU使⽤权，暂时停⽌运⾏。直到线程进⼊就绪状态，才有机会转到运⾏状态。 死亡状态（Dead）：线程执⾏完了或者因异常退出了run⽅法，该线程结束⽣命周期。 阻塞的情况⼜分为三种： 等待阻塞：运⾏的线程执⾏wait⽅法，该线程会释放占⽤的所有资源，JVM会把该线程放⼊“等待池”中。进⼊这个状态后，是不能⾃动唤醒的，必须依靠其他线程调⽤notify或notifyAll⽅法才能被唤醒，wait是object类的⽅法 同步阻塞：运⾏的线程在获取对象的同步锁时，若该同步锁被别的线程占⽤，则JVM会把该线程放⼊“锁池”中。 其他阻塞：运⾏的线程执⾏sleep或join⽅法，或者发出了I&#x2F;O请求时，JVM会把该线程置为阻塞状 态。当sleep状态超时、join等待线程终⽌或者超时、或者I&#x2F;O处理完毕时，线程重新转⼊就绪状态。sleep是Thread类的⽅法 30、并发、并⾏、串⾏之间的区别 串⾏在时间上不可能发⽣重叠，前⼀个任务没搞定，下⼀个任务就只能等着 并⾏在时间上是重叠的，两个任务在同⼀时刻互不⼲扰的同时执⾏。 并发允许两个任务彼此⼲扰。统⼀时间点、只有⼀个任务运⾏，交替执⾏ 31、sleep()、wait()、join()、yield()之间的的区别 锁池：所有需要竞争同步锁的线程都会放在锁池当中，⽐如当前对象的锁已经被其中⼀个线程得到，则其他线程需要在这个锁池进⾏等待，当前⾯的线程释放同步锁后锁池中的线程去竞争同步锁，当某个线程得到后会进⼊就绪队列进⾏等待cpu资源分配。 等待池：当我们调⽤wait（）⽅法后，线程会放到等待池当中，等待池的线程是不会去竞争同步锁。只有调⽤了notify（）或notifyAll()后等待池的线程才会开始去竞争锁，notify（）是随机从等待池选出⼀个线程放到锁池，⽽notifyAll()是将等待池的所有线程放到锁池当中 sleep 是 Thread 类的静态本地⽅法，wait 则是 Object 类的本地⽅法。 sleep⽅法不会释放lock，但是wait会释放，⽽且会加⼊到等待队列中。 sleep⽅法不依赖于同步器synchronized，但是wait需要依赖synchronized关键字。 sleep不需要被唤醒（休眠之后推出阻塞），但是wait需要（不指定时间需要被别⼈中断）。 sleep ⼀般⽤于当前线程休眠，或者轮循暂停操作，wait 则多⽤于多线程之间的通信。 sleep 会让出 CPU 执⾏时间且强制上下⽂切换，⽽ wait 则不⼀定，wait 后可能还是有机会重新竞争到锁继续执⾏的。 yield（）执⾏后线程直接进⼊就绪状态，⻢上释放了cpu的执⾏权，但是依然保留了cpu的执⾏资格，所以有可能cpu下次进⾏线程调度还会让这个线程获取到执⾏权继续执⾏ join（）执⾏后线程进⼊阻塞状态，例如在线程B中调⽤线程A的join（），那线程B会进⼊到阻塞队列，直到线程A结束或中断线程 31、 对线程安全的理解不是线程安全、应该是内存安全，堆是共享内存，可以被所有线程访问，当多个线程访问⼀个对象时，如果不⽤进⾏额外的同步控制或其他的协调操作，调⽤这个对象的⾏为都可以获得正确的结果，我们就说这个对象是线程安全的。 堆是进程和线程共有的空间，分全局堆和局部堆。全局堆就是所有没有分配的空间，局部堆就是⽤户分配的空间。堆在操作系统对进程初始化的时候分配，运⾏过程中也可以向系统要额外的堆，但是⽤完了要还给操作系统，要不然就是内存泄漏。在Java中，堆是Java虚拟机所管理的内存中最⼤的⼀块，是所有线程共享的⼀块内存区域，在虚拟机启动时创建。堆所存在的内存区域的唯⼀⽬的就是存放对象实例，⼏乎所有的对象实例以及数组都在这⾥分配内存。 栈是每个线程独有的，保存其运⾏状态和局部⾃动变量的。栈在线程开始的时候初始化，每个线程的栈互相独⽴，因此，栈是线程安全的。操作系统在切换线程的时候会⾃动切换栈。栈空间不需要在⾼级语⾔⾥⾯显式的分配和释放。 在每个进程的内存空间中都会有⼀块特殊的公共区域，通常称为堆（内存）。进程内的所有线程都可以访问到该区域，这就是造成问题的潜在原因。 32、Thread和Runable的区别Thread和Runnable的实质是继承关系，没有可⽐性。⽆论使⽤Runnable还是Thread，都会newThread，然后执⾏run⽅法。⽤法上，如果有复杂的线程操作需求，那就选择继承Thread，如果只是简单的执⾏⼀个任务，那就实现runnable。 33、 并发的三⼤特性 原子性 关键字：synchronized 原⼦性是指在⼀个操作中cpu不可以在中途暂停然后再调度，即不被中断操作，要不全部执⾏完成，要不都不执⾏。 就好⽐转账，从账户A向账户B转1000元，那么必然包括2个操作：从账户A减去1000元，往账户B加上1000元。2个操作必须全部完成。 可见性 关键字：volatile、synchronized、final 当多个线程访问同⼀个变量时，⼀个线程修改了这个变量的值，其他线程能够⽴即看得到修改的值。 若两个线程在不同的cpu，那么线程1改变了i的值还没刷新到主存，线程2⼜使⽤了i，那么这个i值肯定还是之前的，线程1对变量的修改线程没看到这就是可⻅性问题。 有序性 关键字：volatile、synchronized 虚拟机在进⾏代码编译时，对于那些改变顺序之后不会对最终结果造成影响的代码，虚拟机不⼀定会按照我们写的代码的顺序来执⾏，有可能将他们重排序。实际上，对于有些代码进⾏重排序之后，虽然对变量的值没有造成影响，但有可能会出现线程安全问题。 34、ThreadLocal的底层原理 ThreadLocal是Java中所提供的线程本地存储机制，可以利⽤该机制将数据缓存在某个线程内部，该线程可以在任意时刻、任意⽅法中获取缓存的数据 ThreadLocal底层是通过ThreadLocalMap来实现的，每个Thread对象（注意不是ThreadLocal对象）中都存在⼀个ThreadLocalMap，Map的key为ThreadLocal对象，Map的value为需要缓存的值 如果在线程池中使⽤ThreadLocal会造成内存泄漏，因为当ThreadLocal对象使⽤完之后，应该要把设置的key，value，也就是Entry对象进⾏回收，但线程池中的线程不会回收，⽽线程对象是通过强引⽤指向ThreadLocalMap，ThreadLocalMap也是通过强引⽤指向Entry对象，线程不被回收，Entry对象也就不会被回收，从⽽出现内存泄漏，解决办法是，在使⽤了ThreadLocal对象之后，⼿动调⽤ThreadLocal的remove⽅法，⼿动清楚Entry对象 ThreadLocal经典的应⽤场景就是连接管理（⼀个线程持有⼀个连接，该连接对象可以在不同的⽅法之间进⾏传递，线程之间不共享同⼀个连接） 35、Java死锁如何避免？ 造成死锁的⼏个原因： ⼀个资源每次只能被⼀个线程使⽤ ⼀个线程在阻塞等待某个资源时，不释放已占有资源 ⼀个线程已经获得的资源，在未使⽤完之前，不能被强⾏剥夺 若⼲线程形成头尾相接的循环等待资源关系 这是造成死锁必须要达到的4个条件，如果要避免死锁，只需要不满⾜其中某⼀个条件即可。⽽其中前3个条件是作为锁要符合的条件，所以要避免死锁就需要打破第4个条件，不出现循环等待锁的关系。 在开发过程中： 要注意加锁顺序，保证每个线程按同样的顺序进⾏加锁 要注意加锁时限，可以针对所设置⼀个超时时间 要注意死锁检查，这是⼀种预防机制，确保在第⼀时间发现死锁并进⾏解决 36、如何理解volatile关键字 保证被volatile修饰的共享变量对所有线程总是可⻅的，也就是当⼀个线程修改了⼀个被volatile修饰共享变量的值，新值总是可以被其他线程⽴即得知。 但是不能保证线程安全 37、线程池使用以及相关参数 使用原因 降低资源消耗；提⾼线程利⽤率，降低创建和销毁线程的消耗。 提⾼响应速度；任务来了，直接有线程可⽤可执⾏，⽽不是先创建线程，再执⾏。 提⾼线程的可管理性；线程是稀缺资源，使⽤线程池可以统⼀分配调优监控。 相关参数 corePoolSize 代表核⼼线程数，也就是正常情况下创建⼯作的线程数，这些线程创建后并不会消除，⽽是⼀种常驻线程 maxinumPoolSize 代表的是最⼤线程数，它与核⼼线程数相对应，表示最⼤允许被创建的线程数，⽐如当前任务较多，将核⼼线程数都⽤完了，还⽆法满⾜需求时，此时就会创建新的线程，但是线程池内线程总数不会超过最⼤线程数 keepAliveTime、 unit 表示超出核⼼线程数之外的线程的空闲存活时间，也就是核⼼线程不会消除，但是超出核⼼线程数的部分线程如果空闲⼀定的时间则会被消除,我们可以通过setKeepAliveTime 来设置空闲时间 workQueue ⽤来存放待执⾏的任务，假设我们现在核⼼线程都已被使⽤，还有任务进来则全部放⼊队列，直到整个队列被放满但任务还再持续进⼊则会开始创建新的线程 ThreadFactory 实际上是⼀个线程⼯⼚，⽤来⽣产线程执⾏任务。我们可以选择使⽤默认的创建⼯⼚，产⽣的线程都在同⼀个组内，拥有相同的优先级，且都不是守护线程。当然我们也可以选择⾃定义线程⼯⼚，⼀般我们会根据业务来制定不同的线程⼯⼚ Handler 任务拒绝策略，有两种情况，第⼀种是当我们调⽤ shutdown 等⽅法关闭线程池后，这时候即使线程池内部还有没执⾏完的任务正在执⾏，但是由于线程池已经关闭，我们再继续想线程池提交任务就会遭到拒绝。另⼀种情况就是当达到最⼤线程数，线程池已经没有能⼒继续处理新提交的任务时，这是也就拒绝 38、线程池的底层⼯作原理 如果此时线程池中的线程数量⼩于corePoolSize，即使线程池中的线程都处于空闲状态，也要创建新的线程来处理被添加的任务。 如果此时线程池中的线程数量等于corePoolSize，但是缓冲队列workQueue未满，那么任务被放⼊缓冲队列。 如果此时线程池中的线程数量⼤于等于corePoolSize，缓冲队列workQueue满，并且线程池中的数量⼩于maximumPoolSize，建新的线程来处理被添加的任务。 如果此时线程池中的线程数量⼤于corePoolSize，缓冲队列workQueue满，并且线程池中的数量等于maximumPoolSize，那么通过 handler所指定的策略来处理此任务。 当线程池中的线程数量⼤于 corePoolSize时，如果某线程空闲时间超过keepAliveTime，线程将被终⽌。这样，线程池可以动态的调整池中的线程数 "},{"title":"java -> int 基础类型","date":"2022-02-13T16:00:00.000Z","url":"/2022/02/14/%E6%8A%80%E6%9C%AF/%E5%90%8E%E7%AB%AF/java/bigint%E7%B1%BB%E5%9E%8B/","tags":[["basis","/tags/basis/"]],"categories":[["java","/categories/java/"]],"content":"mysql中bigint、int、mediumint、smallint 和 tinyint的取值范围 引言社区这边的业务就遇到过这个坑，由于是用的开源框架，很多表id的字段用的mediumint类型，随着业务增长，数据量暴增，结果有一天超过id的上限，结果insert db就报错了，影响部分业务功能。 整型数值 整型的每一种都分有无符号（unsigned）和有符号（signed）两种类型，在默认情况下声明的整型变量都是有符号的类型，如果需声明无符号类型的话就需要在类型前加上unsigned。 bigint int mediumint smallint tinyint "},{"title":"jprofiler内存分析工具使用","date":"2022-02-13T16:00:00.000Z","url":"/2022/02/14/%E6%8A%80%E6%9C%AF/%E5%90%8E%E7%AB%AF/java/jprofiler%E5%86%85%E5%AD%98%E5%88%86%E6%9E%90/","tags":[["java","/tags/java/"],["jprofiler","/tags/jprofiler/"]],"categories":[["java","/categories/java/"]],"content":"jprofiler 使用一、分析Heap内存镜像 1.1. 按实例数量排序 1.2. 按占用内存排序 1.3. 向下钻取 得到更相信的信息 二、监控实时运行情况2.1. 在服务区上安装代理 输入服务区root密码 等待连接，然后得到服务区上的java服务列表，就表示代理安装成功了 2.2. 监控服务通过第一步得到的java服务列表，然后选择要监控的服务即可， 这种方式只能扫描到jdk时oracle jdk的服务， 如果是openj9的要用下面的方式 2.3. 监控openj9 服务传 jprofiler_config.xml 文件到root目录， jprofiler_config.xml 在本地的安装路径内查找，默认在 C:\\Program Files\\jprofiler11\\api\\samples\\common 文件夹里 在服务的启动参数里添加一行 注意修改port的值，并在防火墙里打开对应端口 出现这个页面，就是连接成功，直接点 OK 就行了 2.4. 分析实时数据总览 内存 线程 CPU负载 抓取实时内存镜像 可以通过 toString看到对象的值"},{"title":"AngularJS->基本","date":"2022-02-13T16:00:00.000Z","url":"/2022/02/14/%E6%8A%80%E6%9C%AF/%E5%89%8D%E7%AB%AF/AngularJS/AngularJS_%E5%9F%BA%E7%A1%80/","tags":[["AngularJS","/tags/AngularJS/"]],"categories":[["web","/categories/web/"]],"content":"1 . 下载所需要的配置文件以及组件 2 . 开始使用1 . angularjs - 表达式 创建一个html文件 导入所需要的依赖 代码的操作 2 . angluarjs-双向数据绑定 3 . angluarjs-控制器 4 . angluarjs-模块化开发 5 . angluarjs-初始化指令 6 . angluarjs-事件指令 7 . angluarjs-循环数组指令 8 . angluarjs-循环对象数组指令 9 . angluarjs-内置服务1 . 创建一个 json 文件模拟后台数据 2 . 创建一个 html 文件来显示 json 中的内容 10 . 分页功能的演示1 . 创建两个 json 文件来模拟后台传输的数据pagination_1.json pagination_2.json 2 . 创建一个 html 文件来实现"},{"title":"正则表达式->基本语法使用","date":"2022-02-13T16:00:00.000Z","url":"/2022/02/14/%E6%8A%80%E6%9C%AF/%E5%90%8E%E7%AB%AF/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/","tags":[["corn","/tags/corn/"]],"categories":[["java","/categories/java/"]],"content":"正则表达式 在Sun的Java JDK 1.40版本中，Java自带了支持正则表达式的包，主要是放在java.util.regex包下面。 Matcher类的常用方法： matches()：返回整个目标字符串与Pattern是否匹配 find()：返回与Pattern匹配的下一个子串 group()：返回上一次与Pattern匹配的子串中的内容。group是针对（）来说的，group（0）就是指的整个串，group（1） 指的是第一个括号里的东西，group（2）指的第二个括号里的东西 start()：返回上一次与Pattern匹配的子串在目标字符串中的开始位置。 end()：返回上一次与Pattern匹配的子串在目标字符串中的结束位置加1。 正则表达式语法 元字符 描述 \\ 将下一个字符标记符、或一个向后引用、或一个八进制转义符。例如，“\\n”匹配\\n。“\\n”匹配换行符。序列“\\”匹配“\\”而“(”则匹配“(”。即相当于多种编程语言中都有的“转义字符”的概念。 ^ 匹配输入字符串的开始位置。如果设置了RegExp对象的Multiline属性，^也匹配“\\n”或“\\r”之后的位置。 $ 匹配输入字符串的结束位置。如果设置了RegExp对象的Multiline属性，$也匹配“\\n”或“\\r”之前的位置。 * 匹配前面的子表达式任意次。例如，zo能匹配“z”，“zo”以及“zoo”。等价于{0,} + 匹配前面的子表达式一次或多次(大于等于1次）。例如，“zo+”能匹配“zo”以及“zoo”，但不能匹配“z”。+等价于{1,}。 ? 匹配前面的子表达式零次或一次。例如，“do(es)?”可以匹配“do”或“does”中的“do”。?等价于{0,1}。 {n} n是一个非负整数。匹配确定的n次。例如，“o{2}”不能匹配“Bob”中的“o”，但是能匹配“food”中的两个o。 {n,} n是一个非负整数。至少匹配n次。例如，“o{2,}”不能匹配“Bob”中的“o”，但能匹配“foooood”中的所有o。“o{1,}”等价于“o+”。“o{0,}”则等价于“o*”。 {n,m} m和n均为非负整数，其中n&lt;&#x3D;m。最少匹配n次且最多匹配m次。例如，“o{1,3}”将匹配“fooooood”中的前三个o。“o{0,1}”等价于“o?”。请注意在逗号和两个数之间不能有空格。 x|y 匹配x或y。例如，“z [xyz] 字符集合。匹配所包含的任意一个字符。例如，“[abc]”可以匹配“plain”中的“a”。 [^xyz] 负值字符集合。匹配未包含的任意字符。例如，“[^abc]”可以匹配“plain”中的“plin”。 [a-z] 字符范围。匹配指定范围内的任意字符。例如，“[a-z]”可以匹配“a”到“z”范围内的任意小写字母字符。 [^a-z] 负值字符范围。匹配任何不在指定范围内的任意字符。例如，“[^a-z]”可以匹配任何不在“a”到“z”范围内的任意字符。 . 可以匹配任何字符 \\d 匹配一个数字字符。等价于[0-9] \\D 匹配一个非数字字符。等价于[^0-9] \\s 匹配所有的空白字符，包括空格、制表符、换页符、换行符、回车符 等等。等价于[ \\f\\n\\r\\t\\v]。 \\S 匹配所有的非空白字符 更多内容参考： 常用正则表达式 规则 正则表达式语法 一个或多个汉字 ^[\\u0391-\\uFFE5]+$ 邮政编码 ^[1-9]\\d{5}$ QQ号码 ^[1-9]\\d{4,10}$ 用户名（字母开头 + 数字&#x2F;字母&#x2F;下划线） ^[A-Za-z][A-Za-z1-9_-]+$ 手机号码 ^1[3 URL ^((http 18位身份证号 ^(\\d{6})(18 邮箱 ^[a-zA-Z_]{1,}[0-9]{0,}@(([a-zA-z0-9]-*){1,}.){1,3}[a-zA-z-]{1,}$ 示例"},{"title":"git -> 常用命令","date":"2022-02-13T16:00:00.000Z","url":"/2022/02/14/%E6%8A%80%E6%9C%AF/%E5%90%8E%E7%AB%AF/versionControl/git/git-commands/","tags":[["git","/tags/git/"]],"categories":[["versionControl","/categories/versionControl/"]],"content":"git常用命令 git checkout -b develop remotes&#x2F;origin&#x2F;develop git status git add src&#x2F;main&#x2F;java&#x2F;com&#x2F;onlyone&#x2F;csw&#x2F;controllers&#x2F;Test.java git commit -m “备注” git push git pull git checkout 分支名 git merge 分支a git branch -d 分支名 git branch 分支名 git push -u origin 分支名 git push -u origin master git diff topic maste git branch -v -v git reset –hard HEAD~3 git push -f origin 分支名 已经commit 了N次，需要退回到某一版本 git stash git branch git log git log -p "},{"title":"npm、yarn、git设置代理","date":"2022-02-13T16:00:00.000Z","url":"/2022/02/14/%E6%8A%80%E6%9C%AF/other/%E7%AE%A1%E7%90%86%E5%B7%A5%E5%85%B7/npm%E3%80%81yarn%E8%AE%BE%E7%BD%AE%E4%BB%A3%E7%90%86/","tags":[["git","/tags/git/"],["npm","/tags/npm/"],["yarn","/tags/yarn/"]],"categories":[["web","/categories/web/"]],"content":"npm 设置代理无密码的： 有密码的 yarn 设置代理设置代理 ： 删除代理 ： registry 设置 git 设置代理设置代理: 删除代理： "},{"title":"ES->安装","date":"2022-02-11T16:00:00.000Z","url":"/2022/02/12/%E6%8A%80%E6%9C%AF/%E5%90%8E%E7%AB%AF/ElasticSearch/elasticsearch-setup/","tags":[["ElasticSearch","/tags/ElasticSearch/"]],"categories":[["database","/categories/database/"]],"content":"入门及安装 Elasticsearch 是用 Java 编写的，它的内部使用 Lucene 做索引与搜索，但是它的目的是使全文检索变得简单， 通过隐藏 Lucene 的复杂性，取而代之的提供一套简单一致的 RESTful API。 一个 Elasticsearch 集群可以包含多个索引 ，相应的每个索引可以包含多个类型 。 这些不同的类型存储着多个文档 ，每个文档又有多个属性 。 特性 分布式的实时文件存储，每个字段都被索引并可被搜索 分布式的实时分析搜索引擎 可以扩展到上百台服务器，处理PB级结构化或非结构化数据 安装安装非常简单， 下载最新版本的Elasticsearch，其中zip包可直接解压使用。 如何启动 查询索引结构： 使用步骤Elasticsearch 提供了官方客户端，支持市面上象java、javaScript、Groovy、Python等主流语言，下载地址： 入门级案例 "},{"title":"java服务器load飚高排查思路","date":"2022-02-11T16:00:00.000Z","url":"/2022/02/12/%E6%8A%80%E6%9C%AF/%E5%90%8E%E7%AB%AF/liunx/java%E6%9C%8D%E5%8A%A1%E5%99%A8load%E9%A3%9A%E9%AB%98%E6%8E%92%E6%9F%A5%E6%80%9D%E8%B7%AF/","tags":[["Linux","/tags/Linux/"]],"categories":[["Linux","/categories/Linux/"]],"content":"Load 是指对计算机干活多少的度量（WikiPedia：the system load is a measure of the amount of work that a computer system is doing），简单的说是进程队列的长度。Load Average 就是一段时间 (1 分钟、5分钟、15分钟) 内平均 Load 通过uptime命令可以查看当前的load，如果值很高。一般情况是java某些线程长期占用资源、死锁、死循环等导致某个进程占用的CPU资源过高。大致可以从以下几个角度来排查： 1.首先通过ps命令，查看当前进程id，如id为 28174 2.查看该进程下的线程资源使用情况 3.打印JAVA进程28174的堆栈信息 4.将cpu消耗高的线程的pid换算为16进制 转换后的16进制为 0x7fb6 5.从刚才的栈日志中查找该线程正在运行的方法 6.另外也可以查找正在运行的线程，及线程处于运行状态的位置，从这些线程中来查找资源消耗过高的代码。 7、查看当前jvm内存各堆区的占比 jmap -heap 8002 "},{"title":"linux -> 基本命令","date":"2022-02-11T16:00:00.000Z","url":"/2022/02/12/%E6%8A%80%E6%9C%AF/%E5%90%8E%E7%AB%AF/liunx/linux-commands/","tags":[["Linux","/tags/Linux/"]],"categories":[["Linux","/categories/Linux/"]],"content":"linux常用命令 一、CPU相关、进程1、 查看cpu硬件配置 2、 top 命令 实时显示各种系统资源使用情况及进程状态 某一个进程下的线程资源使用情况： 查看系统load、cpu资源的其它命令 3、统计一个进程下的线程数 pstree （以树状图的方式展现进程之间的派生关系）Linux命令大全 pstack （显示每个进程的栈跟踪），也可以查看一个进程下的线程总数 4、查看所有进程 5、对于Java应用从操作系统层面观察，就只有进程和线程两个指标，任何东西在操作系统层面都是以文件的形式存储的，进程也不例外。Linux上部署一个Tomcat程序产生一个进程，这个进程所有的东西都在这个目录下 ll &#x2F;proc&#x2F;{pid}&#x2F; ulimit -a core file size (blocks, -c) 100data seg size (kbytes, -d) unlimitedfile size (blocks, -f) unlimitedpending signals (-i) 15237max locked memory (kbytes, -l) 64max memory size (kbytes, -m) unlimitedopen files (-n) 1024pipe size (512 bytes, -p) 8POSIX message queues (bytes, -q) 819200stack size (kbytes, -s) 8192cpu time (seconds, -t) unlimitedmax user processes (-u) 15237virtual memory (kbytes, -v) unlimitedfile locks (-x) unlimited 二、内存相关1、vmstat Virtual Memory Statistics，统计进程、内存、io、cpu等的活动信息。对于多CPU系统，vmstat打印的是所有CPU的平均输出 注意：排查问题时，要特别关注r的值，如果长时间超过cpu核数2倍，说明系统的负载很重，cpu已经无法及时处理堆积任务。 2、sar -r 3、cat &#x2F;proc&#x2F;meminfo 4、free -m 三、IO及网络1、 tsar –traffic：显示网络带宽 2、 netstat 一般用于检验本机各端口的网络连接情况。netstat是在内核中访问网络及相关信息的程序，它能提供TCP连接，TCP和UDP监听，进程内存管理的相关报告。 命令参数： 输出结果： 找出运行在指定端口的进程 其它使用场景 3、 iostat iostat是I&#x2F;O statistics（输入&#x2F;输出统计）的缩写，主要的功能是对系统的磁盘I&#x2F;O操作进行监视。它的输出主要显示磁盘读写操作的统计信息，同时也会给出CPU使用情况。同vmstat一样，iostat也不能对某个进程进行深入分析，仅对系统的整体情况进行分析。 命令参数： 输出结果： 定时显示所有信息（每隔 2秒刷新显示，且显示3次） 以kB为单位显示所有信息 4、sar -b：磁盘状态历史记录 四、文件1、 lsof (一切皆文件)命令详情 查看你进程开打的文件，打开文件的进程，进程打开的端口(TCP、UDP) 2、 df 3、 du 4、 find 文件查找 5、 tail 从指定点开始将文件标准输出 显示文件最后5行内容 实时显示文件内容 五、用户 六、其它1、查看所有安装的软件包 2、查看环境变量 3、Mac 删除git文件夹，删除svn文件夹 4、 查看进程pid 5、 查看线程信息 6、 系统 已建立的连接数： 处于等待状态的连接数： 各个cpu的资源使用情况： "},{"title":"抓包相关命令","date":"2022-02-11T16:00:00.000Z","url":"/2022/02/12/%E6%8A%80%E6%9C%AF/%E5%90%8E%E7%AB%AF/liunx/linux-%E6%8A%93%E5%8C%85%E7%9B%B8%E5%85%B3%E5%91%BD%E4%BB%A4/","tags":[["Linux","/tags/Linux/"],["tcpdump","/tags/tcpdump/"]],"categories":[["Linux","/categories/Linux/"]],"content":"抓包相关命令抓包分析工具下载 ： 可下载 wireshark 用于分析抓包内容 基本参数 ： 1、监视指定网络接口的数据包 eth1 : 网卡如果不指定网卡，默认tcpdump只会监视第一个网络接口，一般是eth0，下面的例子都没有指定网络接口。 2、 监视指定主机的数据包 打印所有进入或离开sundown的数据包. 也可以指定ip,例如截获所有210.27.48.1 的主机收到的和发出的所有的数据包 截获主机210.27.48.1 和主机210.27.48.2 或210.27.48.3的通信 3、根据 port 进行抓包 捕获 1400 端口的并且网卡为 bond0 的数据 4、根据 ip 和 端口 一起抓包 23 端口 并且 ip 为 10.30.20.61 的， 网卡为 enp0s18 ， 抓包写入到 &#x2F;mnt&#x2F;sry&#x2F;aaa.cap 路径下 23 端口 或者 ip 为 10.30.20.61 的， 网卡为 enp0s18 ， 抓包写入到 &#x2F;mnt&#x2F;sry&#x2F;aaa.cap 路径下 5、调用内容查看 查看 any 所有网卡中 ， 主机为 10.30.20.61 调用的 queryTargetFeign 的 协议内容可用于查看某个ip是否调用过该接口 "},{"title":"ES->基本应用","date":"2022-02-10T16:00:00.000Z","url":"/2022/02/11/%E6%8A%80%E6%9C%AF/%E5%90%8E%E7%AB%AF/ElasticSearch/elasticsearch-application/","tags":[["ElasticSearch","/tags/ElasticSearch/"]],"categories":[["database","/categories/database/"]],"content":"应用场景 基本查询 词条查询。仅匹配在给定字段中含有该词条的文档，而且是确切的、未经分析的词条。 多词条查询。匹配那些在内容中含有某些词条的文档。可以通过设置minimum_match的值来说明想至少保证有多少个词同时被匹配上。 match_all查询。匹配索引中的所有的文件。 常用词查询。考虑到查询条件的词越多，查询性能越低。所以将词分为两类：一类，是重要的词，出现的频率较低；另一类，是出现频率较高，如：”的”，但不那么重要的词。 match查询 multi_match查询。基本与match一样，不同的是它不是针对单个字段，而是针对多个字段执行相同的 match 查询。 match_phrase。精确匹配一系列单词或者短语 。 比如， 我们想执行这样一个查询，仅匹配同时包含 “rock” 和 “climbing” ，并且二者以短语 “rock climbing” 的形式紧挨着的雇员记录。 query_string查询 simple_query_string查询 标识符查询 前缀查询。配置与词条查询类似。如：查询所有的name字段以tom开始的文档。 fuzzy_like_this查询 fuzzy_like_this_field查询 fuzzy查询 通配符查询。允许我们在查询值中使用*和？等通配符。如“cr*me”，表示字段里以cr开头me结尾的文档。 more_like_this查询 more_like_this_field查询 range 范围查询 查询某一个字段值在某一个范围里的文档，字段可以是数值型，也可以是基于字符串的。比如找到年龄在20到30之间的学生。 最大分查询 正则表达式查询 term 查询。 查询被用于精确值 匹配，这些精确值可能是数字、时间、布尔或者那些 not_analyzed 的字符串。而无需对查询结果进行评分计算。 exists 查询和 missing 查询。 用于查找那些指定字段中有值 (exists) 或无值 (missing) 的文档。这与SQL中的 IS_NULL (missing) 和 NOT IS_NULL (exists) 在本质上是相同的。 复合查询 布尔查询 在多个字段上查询多种多样的文本，并且根据一系列的标准来过滤，将多查询组合成单一查询。可以用 bool 查询来实现你的需求。 加权查询 constant_score查询 索引查询 过滤器filter 过滤器不影响评分，只是选择索引中的某个子集。过滤器很容易被缓存，从而进一步提高过滤查询的性能。另外过滤器提供了十几种不同类型，如：范围过滤器、脚本过滤器等等，可以根据不同场景选择合适的。 深入搜索里面提供了多维度、更灵活的搜索场景以及案例。 排序与相关性相关性得分由一个浮点数进行表示，并在搜索结果中通过 _score 参数返回， 默认排序是 _score 降序。 按单个字段的值排序 多级排序 比如我们想要结合使用 date 和 _score 进行查询，并且匹配的结果首先按照日期排序，然后按照相关性排序。 。多级排序并不一定包含 _score 。你可以根据一些不同的字段进行排序， 如地理距离或是脚本计算的特定值。 基于poi经纬度地理位置的查询 基于距离的排序。按照与给定地点的距离来对结果排序。 边界框过滤。搜索条件提供左上及右下的坐标，搜索被矩形框住的选定区域。 距离的限制。把结果限定为离基准点一个选定的距离之内，比如把结果限定为离巴黎半径500公里以内。 "},{"title":"ES->基本语法使用","date":"2022-02-10T16:00:00.000Z","url":"/2022/02/11/%E6%8A%80%E6%9C%AF/%E5%90%8E%E7%AB%AF/ElasticSearch/es_%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95%E4%BD%BF%E7%94%A8/","tags":[["ElasticSearch","/tags/ElasticSearch/"]],"categories":[["database","/categories/database/"]],"content":"ES 查询相关内容基础变量信息 注 ： 复制 json 注意删除注释 1、 table 操作相关1.1、 查看表信息 查看表信息(相关配置项) 1.2、 aliases 查看所有表别名 1.3、count 查看库中数据数量 1.4、 indices 获取所有表的信息 获取指定表信息 1.5、 refresh 刷新索引修改 ES 数据之后，不会及时刷新。故可能会调用这个index settings 配置文件中 index.refresh_interval 来指定refresh间隔 1.6、 获取指定表信息 获取指定表信息 1.7、 新建表 新建表 json 1.8、 清空表数据 清空表数据 json 1.9、添加表字段 添加表字段 json 1.9、 删除库 删除库 2、 表配置相关2.1、 线程池 线程池 2.2、 查询设置 查询设置 2.3、 修改配置 修改配置 json 3、 查询相关3.1、 _doc (主键id查询) 主键id查询 3.2、 普通查询 普通查询 条数查询 json 查询单个字段 jsonwarn_code ： 字段名0606009 ： 实际值 查询多个字段 jsonsfsc ： 字段名0 ： 实际值 时间类查询 jsondt_create_time : 字段名gte ： 大于lte ： 小于2020-02-13T13:13:09 ： 实际时间 模糊查询 json【wildcard】 用于分词查询注意分词查询需要前后 + * 排序 json【sort】 排序dt_create_time ： 字段名desc : asc | 正序、倒序 模糊分词查询 json【match_phrase】 非分词情况下的查询s_extends_info ： 字段名$alarmId ： 实际查询内容 3.3、 批量操作 批量操作 json 3.4、 特殊查询(date_histogram) 特殊查询（date_histogram）用于查询查询 。例如统计按照每年每月，或者没小时统计是否需要自动补 0 【min_doc_count】 配合 【extended_bounds】 使用 4 、 分词查询4.1 、 测试分词 测试分词 5、 ES 备份还原数据 ！！！ es 数据备份还原TODO - 待补充 "},{"title":"内存溢出env配置项","date":"2022-02-10T16:00:00.000Z","url":"/2022/02/11/%E6%8A%80%E6%9C%AF/%E5%90%8E%E7%AB%AF/java/%E5%86%85%E5%AD%98%E6%BA%A2%E5%87%BA%E6%8E%92%E6%9F%A5/","tags":[["liunx","/tags/liunx/"]],"categories":[["liunx","/categories/liunx/"]],"content":"1. env 文件中添加以下内容进行查看内存问题-XX:HeapDumpPath&#x3D;.&#x2F;dumpfile.hprof -XX:+HeapDumpOnOutOfMemoryError"}]